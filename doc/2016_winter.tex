%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\usepackage{tikz}
\usepackage{ifthen}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Missing data seminar]{Variable selection: From full data to missing data} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[Yongchan Kwon]{Yongchan Kwon} % Your name
\institute[] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
\textit{Department of Statistics, Seoul National University, Seoul,  Korea}
}
\date{December 8, 2014} % Date, can be changed to a custom date

\newcommand{\blockmatrix}[9]{
  \draw[draw=#4,fill=#5] (0,0) rectangle( #1,#2);
  \ifthenelse{\equal{#6}{true}}
  {
    \draw[draw=#7,fill=#8] (0,#2) -- (#9,#2) -- ( #1,#9) -- ( #1,0) -- ( #1 - #9,0) -- (0,#2 -#9) -- cycle;
  }
  {}
  \draw ( #1/2, #2/2) node { #3};
}

\newcommand{\mblockmatrix}[4][none]{
  \begin{tikzpicture} 
  \ifthenelse{\equal{#1}{none}}
  {
    \blockmatrix{#2}{#3}{#4}{none}{none}{false}{none}{none}{0.0}
  }
  {
    \definecolor{fillcolor}{rgb}{#1}
    \blockmatrix{#2}{#3}{#4}{none}{fillcolor}{false}{none}{none}{0.0}
  }
  \end{tikzpicture}%this comment is necessary
}

\newcommand{\fblockmatrix}[4][none]{
  \begin{tikzpicture} 
  \ifthenelse{\equal{#1}{none}}
  {
    \blockmatrix{#2}{#3}{#4}{black}{none}{false}{none}{none}{0.0}
  }
  {
    \definecolor{fillcolor}{rgb}{#1}
    \blockmatrix{#2}{#3}{#4}{black}{fillcolor}{false}{none}{none}{0.0}
  }
  \end{tikzpicture}%this comment is necessary
}
\newenvironment{blockmatrixtabular}
{% necessary comment
  \begin{tabular}{
  @{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l
  @{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l
  @{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l
  @{}
  }
}
{
  \end{tabular}%necessary comment
}



\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Contents} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
% Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
\section{Introduction}
\section{Classical variable selection}
\section{Penalized least squares method}
\section{Variable selection in missing data}
\section{References}
%------------------------------------------------

%\subsection{Subsection Example} % A subsection can be created just before a set of slides with a common theme to further break down your presentation into chunks

\begin{frame}
\frametitle{Introduction}
\centering{
\begin{blockmatrixtabular}
\valignbox{\fblockmatrix                    {0.15in}{0.6in}{$y$}}&
\valignbox{\mblockmatrix                    {0.15in}{0.6in}{$=$}}&
\valignbox{\fblockmatrix       [0.8,1.0,0.8]{0.5in}{0.6in}{$X$}}&
\valignbox{\fblockmatrix       [0.8,0.8,1.0]{0.15in}{0.5in}{$\beta$}}\\
\end{blockmatrixtabular}
}
\\
\vspace{0.7in}
$\to$ Is variable selection important ??
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Introduction}
\centering{
\begin{blockmatrixtabular}
\valignbox{\fblockmatrix                    {0.15in}{0.6in}{$y$}}&
\valignbox{\mblockmatrix                    {0.15in}{0.6in}{$=$}}&
\valignbox{\fblockmatrix       [0.8,1.0,0.8]{0.5in}{0.6in}{$X_S$}}&
\valignbox{\fblockmatrix       [0.8,1.0,0.8]{1.0in}{0.6in}{$X_U$}}&
\valignbox{\fblockmatrix       [0.8,0.8,1.0]{0.15in}{0.5in}{$\beta_S$}}\\
&&&&\valignbox{\fblockmatrix       [0.8,0.8,1.0]{0.15in}{1.0in}{$\beta_U$}}
\end{blockmatrixtabular}
}
\\
\vspace{0.2in}
$\to$ Yes! variable selection is so important !
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Classical variable selection}

\begin{block}{ The best subset selection}
[step1] Define $\mathcal{M}_k$ be the set of all linear functions with $k$ nonzero coefficients. 

\noindent [step2] For $k=0, \cdots , p$, choose $m_k \in \mathcal{M}_k$ such that $m_k$ has the minimum of  RSS($\beta$) $=  (y - X \beta)^T (y - X \beta)$ among $\mathcal{M}_k$.

\noindent [step3] Among $m_0, \cdots, m_p$, choose one model using cross-validation, AIC, or BIC.
\end{block}
$\to$ Can be costly in computation. 
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Penalized least squares method}
Let $y$ be an $n \times 1 $ vector and $ X$ be an $n \times d$ matrix. Then, a form of the penalized least squares is for $\lambda >0$ 
$$\rm{argmin}_{\beta}  \left( \frac{1}{2} (y - X \beta)^T (y - X \beta) +  \sum_{j=1} ^d p_\lambda ( |\beta_j| ) \right)$$
where $ p_\lambda ( \cdot ) $ is called a penalty function indexed up to penalty parameter $\lambda$. Penalty parameter $\lambda$ can be chosen by Generalized Cross-Validation(GCV).\\ \vspace{0.2in}

$L_q$ penalty : $ p_{\lambda j} (|\beta_j|) = \lambda |\beta_j| ^q $

$q=2$ : Ridge regression $\to$ No variable selection features

$q=1$ : LASSO
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Penalized least squares method}
Fan and Li (2001) suggest that a good penalty function should result in an estimator with three properties.
\begin{block}{To be a good estimator.....}
1.\textbf{Unbiasedness:} The resulting estimator is nearly unbiased when the true unknown parameter is large to avoid unnecessary modeling bias.\\
2.\textbf{Sparsity:} The resulting estimator is a thresholding rule, which automatically sets small estimated coefficient to zero to reduce model complexity.\\
3.\textbf{Continuity:} The resulting estimator is continuous in the data to avoid instability in model prediction.
\end{block}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Penalized least squares method}
 Fan and Li (2001) proposed Smoothly Clipped Absolute Deviation (SCAD) penalty defined by
$$
p\rq{} _\lambda ( \beta ) =\lambda \left\{ I(\beta \leq \lambda) + \frac{(a \lambda - \beta)_{+}}{(a-1) \lambda} I(\beta > \lambda) \right\} 
$$
for some $a>2$ and $\beta>0$.

\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Penalized least squares method}
\begin{figure}[h]
\centering
\includegraphics[natwidth=162bp, natheight=227bp, width=200bp]{penalty.png}
\caption{Comparing $L_1$, $L_2$, and SCAD penalty functions}
\end{figure}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Penalized least squares method}
\begin{block}{Oracle property}
 Let $\beta^*$ be the true regression coefficient and $A= \{ j : \beta^* _j  \neq 0 \} $. We will say $\beta^o$ be the oracle estimator defined as
$$
\beta^o = \rm{argmin}_{\beta , \beta_j = 0, j \in A^c} \frac{1}{2} (y - X \beta)^T (y - X \beta)
$$
 $\hat{\beta}$ is said to possess \textbf{the oracle property} if there exists a sequence of $\lambda_n$ such that with $\lambda = \lambda_n$  $$ \lim_n \rm{Pr}( \hat{\beta}  = \beta^0 ) =1 .$$

A slightly weaker definition is that if estimator satisfies 

(1)	$ \lim_n \rm{Pr}( \hat{A }  = A^* ) =1 $

(2)	$ \sqrt{n} ( \hat{\beta} - \beta^*  ) \overset{d}= \sqrt{n} ( \beta^o - \beta^*  ) . $
\end{block}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Variable selection in missing data}
\begin{block}{Setting}
$(X_1, z_1, y_1), \cdots, (X_n, z_n, y_n)$ : $n$ independent observations\\
$y_i$ : the response variable\\
$X_i$ : a completely observed covariates.\\
$z_i$ : a partially observed covariates.\\
 ($z_{m.i}, z_{o.i}$) : missing and observed component of $z_i$.\\
$r_i$ : response indicator for $z_i$.\\
$D_{f,i}$ and $D_{o,i}$ : full and observed data of subject $i$\\
$D_{f}$ and $D_{o}$ :  the entire full and observed data \\
$D_{m}$ : missing part.
\end{block}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Variable selection in missing data}
\begin{block}{Setting2}
Then,
$$
f(D_c) = \prod _{i=1} ^n f( y_i, z_i, r_i \mid x_i , \eta ) 
$$
Where $\eta$ is a parameter. According to the EM algorithm, we define Q-function given by
$$
Q(\eta \mid \eta^{(s)} ) = E[ \log f(D_{f}; \eta) \mid D_{o} ; \eta^{(s)} ]. 
$$
By definition, we can write
$$
Q(\eta \mid \eta^{(s)} ) = \log f(D_{o}; \eta)  + H(\eta \mid \eta^{(s)} )
$$
Where $ H(\eta \mid \eta^{(s)} ) = E[ \log f(D_{m} \mid D_{o} ; \eta) \mid D_{o} ; \eta^{(s)} ] $.
\end{block}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Variable selection in missing data}
Ibrahim, Zhu, and Tang (2008) give an idea to calculate observed likelihood by approximating H-function using a truncated Hermite expansion.(One of orthogonal series expansion.) 

In the same paper, they define two new information criteria given by
$$
IC_{H,Q} = -2 \log f(D_{o}; \hat{\eta}) + c_n (\hat{\eta} ) = -2 Q(\hat{\eta} \mid \hat{\eta}) +2 H(\hat{\eta} \mid \hat{\eta}) +  c_n (\hat{\eta} )  
$$
$$
IC_{Q} =-2 Q(\hat{\eta} \mid \hat{\eta}) +  c_n (\hat{\eta} )  
$$
where $c_n (\hat{\eta})$ is a function of the data and the fitted model. By choosing small $IC_{H,Q}$, we can select the model(variable selection).
For instance, if $c_n (\hat{\eta})= dim(\eta) \times 2$ is an AIC-type criterion.
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Variable selection in missing data}
Thus, penalized idea is revisited!! Garcia, Ibrahim, and Zhu (2010) proposed the method to develop variable selection with penalty function for missing data problems.
\begin{block}{Idea!!}
The idea is that \\
(1) parameter is estimated by penalized likelihood method \\
(2) penalty parameter is chosen by minimizing $IC_{Q}$.
\end{block}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Variable selection in missing data}
\begin{block}{Assumptions}
(A1) $\eta^*$ is unique and an interior point of the compact parameter space $\Theta$.

(A2) $\hat{\eta}_o \to \eta^*$  in probability.

(A3) For all $i, l_i (\eta)$ is three-times continuously differentiable on $\Theta$ and $l_i(\eta), |\partial_j l_i(\eta) | ^2$ and $|\partial_j \partial_k \partial_l l_i(\eta) | $ are dominated by $B_i (D_{o,i})$ for all $j,k,l = 1, \cdots, d$. where $d$ is a number of candidate covariates and $\partial_j = \partial / \partial_j$.

(A4) For each $\epsilon >0 $, there exists a finite $K$ such that
$$
sup_{n \geq1} \frac{1}{n} \sum_{i=1} ^n E [ B_i(D_{o,i}) 1_{B_i(D_{o,i})>K}]  < \epsilon
$$
for all $n$.
\end{block}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Variable selection in missing data}
\begin{block}{Assumptions}
(A5) \begin{align*}
\lim_{n} -\frac{1}{n} \sum_{i=1} ^n \partial _\eta ^2 l_i(\eta^*) &= A(\eta^*) \\
\lim_{n} \frac{1}{n} \sum_{i=1} ^n \partial _\eta  l_i(\eta^*) \partial _\eta l_i(\eta^*)^T &= B(\eta^*)\\
\lim_{n} -\frac{1}{n} \sum_{i=1} ^n D^{20}Q(\eta_S ^* | \eta^*) &= C(\eta_S ^* | \eta^*)\\
\lim_{n} \frac{1}{n} \sum_{i=1} ^n D^{10}Q(\eta_S ^* | \eta^*) D^{10}Q(\eta_S ^* | \eta^*)^T &= D(\eta_S ^* | \eta^*)
\end{align*}
where $A(\eta^*)$ and $C(\eta_S ^* | \eta^*)$ are positive definite and $D^{ij}$ denotes the $i$-th and $j$-th derivatives of the first and second component of the Q function.
\end{block}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Variable selection in missing data}
\begin{block}{Assumptions}
(A6) Define $a_n = \rm{max}_j \{ p\rq{} _{\lambda_{j_n}} (| \beta^* _j |) : \beta^* _j  \neq 0\}$, and $b_n = \rm{max}_j \{ p\rq{}\rq{} _{\lambda_{j_n}} (| \beta^* _j |) : \beta^* _j  \neq 0\}$\\
1. $\rm{max}_j \{ \lambda_{j_n} :  \beta^* _j  \neq 0\} = o_p (1)$ \\
2. $a_n = O_p (n^{-1/2})$.\\
3. $b_n = o_p (1)$.

(A7) Define $d_n = \rm{min}_j \{ \lambda_{j_n} :  \beta^* _j  = 0\}.$\\
1. For all $j$ such that $\beta_j ^* = 0$, $\lim _n \lambda_{j_n} ^{-1} \liminf _{\beta \to 0+} p\rq{} _{ \lambda_{j_n}} (\beta) >0 $ in probability.\\
2. $ n^{1/2}d_n \overset{p}\to \infty $. 

\end{block}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Variable selection in missing data}
\begin{theorem} Under assumptions (A1)-(A7), we have\\
(1) Unbiasedness: $\hat{\eta}_\lambda - \eta^* = O_p (n^{-1/2})$ as $n \to \infty$.\\
(2) Sparsity: $P( \hat{\beta}_{(2) \lambda} = 0 ) \to 1$. \\ 
(3) Asymptotic normality: $(\hat{\beta}_{(1)\lambda}, \hat{\tau_{\lambda}}, \hat{\alpha_{\lambda}},\hat{\zeta_{\lambda}} ) $ is asymptotically normal.
\end{theorem}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Variable selection in missing data}

proof of (1). Given assumptions, then it follows from White (1994) that
$$
n^{-1/2} \sum_{i=1} ^{n} \partial _\eta  l_i(\eta^*) \overset{D}\to N(0, B(\eta^*)).
$$
and
$$
n^{1/2}(\hat{\eta}_o - \eta^*) \overset{D}\to N(0, A(\eta^*) ^{-1} B(\eta^*)A(\eta^*) ^{-1} )
$$
To show $\hat{\eta}_\lambda$ is a $\sqrt{n}$-consistent maximizer of $\eta^*$, it is enough to show that for large $C$
\begin{align*}
P  \Big( \sup_{|u| =C} \left\{  l(\eta^* + n^{-1/2} u ) - n \sum_{j=1} ^p p_{\lambda_{j_n}} (|\beta_j ^* + n^{-1/2} u_j |)  \right\}\\ <  l(\eta^*) +n \sum_{j=1} ^p p_{\lambda_{j_n}} (|\beta_j ^* |)  \Big) \to 1
\end{align*}

\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Variable selection in missing data}

Since this implies there exists a local maximizer in the ball $\{ \eta^* + n^{-1/2} u; |u| <C  \}$ and thus unbiasedness is proved. Taking a Taylor\rq{}s expansion of the penalized likelihood function, we have
\begin{align*}
& l(\eta^* + n^{-1/2} u )  -  l(\eta^*) +n \sum_{j=1} ^p p_{\lambda_{j_n}} (|\beta_j ^* |) - n \sum_{j=1} ^p p_{\lambda_{j_n}} (|\beta_j ^* + n^{-1/2} u_j |) \\
&\leq n^{-1/2} u^T \partial_{\eta} l(\eta^*) - \frac{1}{2} u^T A(\eta^*) u +\sqrt{p_1}n^{1/2}a_n |u|-\frac{1}{2}|b_n| |u|^2 +o_p(1) \\
&\leq n^{-1/2} u^T \partial_{\eta} l(\eta^*) - \frac{1}{2} u^T A(\eta^*) u +\sqrt{p_1}n^{1/2}a_n |u| +o_p(1)
\end{align*}
Note that except the second term of last equation is $O_p(1)$ and $  u^T A(\eta^*) u$ is bounded below by $|u| ^2 \times $ the smallest eigenvalue of $ A(\eta^*)  $, then this dominates other three terms. Thus, results can be made negative for enough large $C$.

\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{References}
\begin{thebibliography}{99} % Beamer does not support BibTeX so references must be inserted manually as below
\bibitem{Cd94} Fan, J., and Li, R. (2001), \lq\lq{}Variable selection via nonconcave penalized likelihood and its oracle properties\rq\rq{}, Journal of the American Statistical Association, Dec 2001, Vol. 96, No. 456.
\bibitem{Cd94} Zou, H. (2006), \lq\lq{}The Adaptive Lasso and its oracle properties\rq\rq{}, Journal of the American Statistical Association, Dec 2006, Vol. 101, No. 476. 
\bibitem{Cd94} Ibrahim, J. G., Zhu, H., and Tang, N. (2008), \lq\lq{}Model selection Criteria for missing data problems using the EM algorithm\rq\rq{}, Journal of the American Statistical Association, Dec 2008, Vol. 103, No. 484.
\end{thebibliography}
\end{frame}
\begin{frame}
\frametitle{References}
\begin{thebibliography}{99} % Beamer does not support BibTeX so references must be inserted manually as below
\bibitem{Cd94} Garcia, R. I., Ibrahim, J. G., and Zhu, H. (2010), \lq\lq{}Variable selection for regression models with missing data\rq\rq{}, Statistica Sinica, 20 (2010), 149-465.
\bibitem{Cd94} J. Fan, and J. Lv. (2010), \lq\lq{}A selective overview of variable selection in High Dimensional Feature Space\rq\rq{}, Stat Sin. 2010 January; 20(1): 101-148.
\bibitem{Cd94} Tibshirani, R. (1996), \lq\lq{}Regression shrinkage and selection via the Lasso\rq\rq{}, Journal of the Royal Statistical Society, Series B, Volume 58, Issue 1 (1996), 267-288.
\end{thebibliography}
\end{frame}

%------------------
\end{document}








